## ç®€ä»‹

Traineræ¨¡å—æ˜¯åŸºç¡€ç»„ä»¶çš„æœ€åä¸€ä¸ªæ¨¡å—ï¼Œå®ƒå°è£…äº†ä¸€å¥—å®Œæ•´çš„åœ¨æ•°æ®é›†ä¸Šè®­ç»ƒã€è¯„ä¼°ä¸é¢„æµ‹çš„æµç¨‹ã€‚

å€ŸåŠ©Traineræ¨¡å—ï¼Œå¯ä»¥å¿«é€Ÿå¯åŠ¨è®­ç»ƒã€‚

Traineræ¨¡å—ä¸»è¦åŒ…å«ä¸¤éƒ¨åˆ†çš„å†…å®¹ï¼šTrainingArgumentsä¸Trainerï¼Œå‰è€…ç”¨äºè®­ç»ƒå‚æ•°çš„è®¾ç½®ï¼Œåè€…ç”¨äºåˆ›å»ºçœŸæ­£çš„è®­ç»ƒå™¨ï¼Œè¿›è¡Œè®­ç»ƒã€è¯„ä¼°é¢„æµ‹ç­‰å®é™…æ“ä½œã€‚

æ­¤å¤–ï¼Œé’ˆå¯¹Seq2Seqè®­ç»ƒä»»åŠ¡ï¼Œæä¾›äº†ä¸“é—¨çš„Seq2SeqTrainingArgumentsä¸Seq2SeqTrainerï¼Œæ•´ä½“ä¸TrainingArgumentså’ŒTrainerç±»ä¼¼ï¼Œä½†æ˜¯æä¾›äº†ä¸“é—¨ç”¨äºç”Ÿæˆçš„éƒ¨åˆ†å‚æ•°ã€‚

## TrainingArguments

TrainingArgumentsä¸­å¯ä»¥é…ç½®æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„å‚æ•°ï¼Œé»˜è®¤ç‰ˆæœ¬æ˜¯åŒ…å«90ä¸ªå‚æ•°ï¼Œæ¶‰åŠæ¨¡å‹å­˜å‚¨ã€æ¨¡å‹ä¼˜åŒ–ã€è®­ç»ƒæ—¥å¿—ã€GPUä½¿ç”¨ã€æ¨¡å‹ç²¾åº¦ã€åˆ†å¸ƒå¼è®­ç»ƒç­‰å¤šæ–¹é¢çš„é…ç½®å†…å®¹ï¼Œè¿™é‡Œå°±ä¸ä¸€ä¸€ä»‹ç»äº†ï¼Œ

![img](https://pic1.zhimg.com/v2-7cb0cfe598e04cac8d348e8caf657624_r.jpg)

Seq2SeqTrainingArgumentsä¸­é™¤äº†ä¸Šè¿°çš„å†…å®¹è¿˜åŒ…æ‹¬ç”Ÿæˆéƒ¨åˆ†çš„å‚æ•°è®¾ç½®ï¼Œå¦‚æ˜¯å¦è¦è¿›è¡Œç”Ÿæˆã€æœ€å¤§é•¿åº¦ç­‰å…±94ä¸ªå‚æ•°ã€‚

## Trainer

Trainerä¸­é…ç½®å…·ä½“çš„è®­ç»ƒç”¨åˆ°çš„å†…å®¹ï¼ŒåŒ…æ‹¬æ¨¡å‹ã€è®­ç»ƒå‚æ•°ã€è®­ç»ƒé›†ã€éªŒè¯é›†ã€åˆ†è¯å™¨ã€è¯„ä¼°å‡½æ•°ç­‰å†…å®¹ã€‚

å½“æŒ‡å®šå®Œä¸Šè¿°å¯¹åº”å‚æ•°ï¼Œä¾¿å¯ä»¥é€šè¿‡è°ƒç”¨trainæ–¹æ³•è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼›è®­ç»ƒå®Œæˆåå¯ä»¥é€šè¿‡è°ƒç”¨evaluateæ–¹æ³•å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼›å¾—åˆ°æ»¡æ„çš„æ¨¡å‹åï¼Œæœ€åè°ƒç”¨predictæ–¹æ³•å¯¹æ•°æ®é›†è¿›è¡Œé¢„æµ‹ã€‚

```python
from transformers import TrainingArguments, Trainer
# åˆ›å»ºTrainingArguments
training_args = TrainingArguments(...)
# åˆ›å»ºTrainer
trainer = Trainer(..., args=training_args, ...)
# æ¨¡å‹è®­ç»ƒ
trainer.train()
# æ¨¡å‹è¯„ä¼°
trainer.evaluate()
# æ¨¡å‹é¢„æµ‹
trainer.predict()
```

ä¸è¿‡ï¼Œå¦‚æœåœ¨åˆ›å»ºTrainerå¯¹è±¡æ—¶æ²¡æœ‰æŒ‡å®šè¯„ä¼°å‡½æ•°ï¼Œé‚£ä¹ˆè°ƒç”¨evaluateæ–¹æ³•æ—¶åªèƒ½å±•ç¤ºlossçš„ä¿¡æ¯ã€‚

**éœ€è¦ç‰¹åˆ«æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨Trainerè¿›è¡Œæ¨¡å‹è®­ç»ƒå¯¹æ¨¡å‹çš„è¾“å…¥è¾“å‡ºæ˜¯æœ‰é™åˆ¶çš„ï¼Œè¦æ±‚æ¨¡å‹è¿”å›ModelOutputçš„å…ƒç»„æˆ–å­ç±»ï¼ŒåŒæ—¶å¦‚æœæä¾›äº†æ ‡ç­¾ï¼Œæ¨¡å‹è¦èƒ½è¿”å›lossç»“æœï¼Œå¹¶ä¸”lossè¦ä½œä¸ºModelOutputå…ƒç»„çš„ç¬¬ä¸€ä¸ªå€¼**ã€‚

![image-20230509175119276](C:\Users\Administrator\Desktop\github\python_common_code_collection\src\huggingface\6.trainer.assets\image-20230509175119276.png)

ä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬å¯ä»¥è‡ªå®šä¹‰Trainerï¼Œç»§æ‰¿Traineråé‡å†™ä¸Šè¿°çš„ä¸€äº›æ–¹æ³•ã€‚

æˆ‘ä»¬å¯ä»¥è‡ªå®šä¹‰æŸå¤±è®¡ç®—ï¼š

```python
from torch import nn
from transformers import Trainer


class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get("labels")
        # forward pass
        outputs = model(**inputs)
        logits = outputs.get("logits")
        # compute custom loss (suppose one has 3 labels with different weights)
        loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 2.0, 3.0]))
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        return (loss, outputs) if return_outputs else loss
```

ä¸€äº›å‚æ•°ï¼š

- **model** ([PreTrainedModel](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/model#transformers.PreTrainedModel) or `torch.nn.Module`, *optional*) â€” The model to train, evaluate or use for predictions. If not provided, a `model_init` must be passed.[Trainer](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.Trainer) is optimized to work with the [PreTrainedModel](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/model#transformers.PreTrainedModel) provided by the library. You can still use your own models defined as `torch.nn.Module` as long as they work the same way as the ğŸ¤— Transformers models.
- **args** ([TrainingArguments](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.TrainingArguments), *optional*) â€” The arguments to tweak for training. Will default to a basic instance of [TrainingArguments](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.TrainingArguments) with the `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.
- **data_collator** (`DataCollator`, *optional*) â€” The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will default to [default_data_collator()](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/data_collator#transformers.default_data_collator) if no `tokenizer` is provided, an instance of [DataCollatorWithPadding](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/data_collator#transformers.DataCollatorWithPadding) otherwise.
- **train_dataset** (`torch.utils.data.Dataset` or `torch.utils.data.IterableDataset`, *optional*) â€” The dataset to use for training. If it is a [Dataset](https://huggingface.co/docs/datasets/v2.11.0/en/package_reference/main_classes#datasets.Dataset), columns not accepted by the `model.forward()` method are automatically removed.Note that if itâ€™s a `torch.utils.data.IterableDataset` with some randomization and you are training in a distributed fashion, your iterable dataset should either use a internal attribute `generator` that is a `torch.Generator` for the randomization that must be identical on all processes (and the Trainer will manually set the seed of this `generator` at each epoch) or have a `set_epoch()` method that internally sets the seed of the RNGs used.
- **eval_dataset** (Union[`torch.utils.data.Dataset`, Dict[str, `torch.utils.data.Dataset`]), *optional*) â€” The dataset to use for evaluation. If it is a [Dataset](https://huggingface.co/docs/datasets/v2.11.0/en/package_reference/main_classes#datasets.Dataset), columns not accepted by the `model.forward()` method are automatically removed. If it is a dictionary, it will evaluate on each dataset prepending the dictionary key to the metric name.
- **tokenizer** ([PreTrainedTokenizerBase](https://huggingface.co/docs/transformers/v4.28.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase), *optional*) â€” The tokenizer used to preprocess the data. If provided, will be used to automatically pad the inputs to the maximum length when batching inputs, and it will be saved along the model to make it easier to rerun an interrupted training or reuse the fine-tuned model.
- **model_init** (`Callable[[], PreTrainedModel]`, *optional*) â€” A function that instantiates the model to be used. If provided, each call to [train()](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.Trainer.train) will start from a new instance of the model as given by this function.The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to be able to choose different architectures according to hyper parameters (such as layer count, sizes of inner layers, dropout probabilities etc).
- **compute_metrics** (`Callable[[EvalPrediction], Dict]`, *optional*) â€” The function that will be used to compute metrics at evaluation. Must take a [EvalPrediction](https://huggingface.co/docs/transformers/v4.28.1/en/internal/trainer_utils#transformers.EvalPrediction) and return a dictionary string to metric values.
- **callbacks** (List of [TrainerCallback](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/callback#transformers.TrainerCallback), *optional*) â€” A list of callbacks to customize the training loop. Will add those to the list of default callbacks detailed in [here](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/callback).If you want to remove one of the default callbacks used, use the [Trainer.remove_callback()](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#transformers.Trainer.remove_callback) method.
- **optimizers** (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`, *optional*) â€” A tuple containing the optimizer and the scheduler to use. Will default to an instance of [AdamW](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/optimizer_schedules#transformers.AdamW) on your model and a scheduler given by [get_linear_schedule_with_warmup()](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/optimizer_schedules#transformers.get_linear_schedule_with_warmup) controlled by `args`.
- **preprocess_logits_for_metrics** (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`, *optional*) â€” A function that preprocess the logits right before caching them at each evaluation step. Must take two tensors, the logits and the labels, and return the logits once processed as desired. The modifications made by this function will be reflected in the predictions received by `compute_metrics`.

é‡è¦çš„ä¸€äº›å±æ€§ï¼š

Important attributes:

- **model** â€” Always points to the core model. If using a transformers model, it will be a [PreTrainedModel](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/model#transformers.PreTrainedModel) subclass.
- **model_wrapped** â€” Always points to the most external model in case one or more other modules wrap the original model. This is the model that should be used for the forward pass. For example, under `DeepSpeed`, the inner model is wrapped in `DeepSpeed` and then again in `torch.nn.DistributedDataParallel`. If the inner model hasnâ€™t been wrapped, then `self.model_wrapped` is the same as `self.model`.
- **is_model_parallel** â€” Whether or not a model has been switched to a model parallel mode (different from data parallelism, this means some of the model layers are split on different GPUs).
- **place_model_on_device** â€” Whether or not to automatically place the model on the device - it will be set to `False` if model parallel or deepspeed is used, or if the default `TrainingArguments.place_model_on_device` is overridden to return `False` .
- **is_in_train** â€” Whether or not a model is currently running `train` (e.g. when `evaluate` is called while in `train`)

ä¹Ÿå°±æ˜¯è¯´é™¤äº†æ•°æ®å¹¶è¡Œä¹‹å¤–ï¼Œè¿˜å¯ä»¥è¿›è¡Œæ¨¡å‹å¹¶è¡Œã€‚

æ›´å¤šè¯·å‚è€ƒï¼š[Trainer (huggingface.co)](https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/trainer#trainer)