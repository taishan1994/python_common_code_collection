## åŠ è½½åˆ†è¯å™¨

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
```

## å¥å­åˆ†è¯

```python
sen = "å¼±å°çš„æˆ‘ä¹Ÿæœ‰å¤§æ¢¦æƒ³"
tokens = tokenizer.tokenize(sen)
```

## æŸ¥çœ‹vocab

```python
tokenizer.vocab
```

## è½¬æ¢

```python
ids = tokenizer.convert_tokens_to_ids(tokens)
tokens = tokenizer.convert_ids_to_tokens(ids)
```

## ç¼–ç 

```python
encode_dict = tokenizer.encode(sen, 
                       padding="max_length", 
                       max_length=15, 
                       truncation=True, 
                       return_attention_mask=True,
                       return_token_type_ids=True)
```

æˆ–è€…

```python
tokens_a = 'æˆ‘ çˆ± åŒ— äº¬ å¤© å®‰ é—¨'.split(' ')
tokens_b = 'æˆ‘ çˆ± æ‰“ è‹± é›„ è” ç›Ÿ å•Š å•Š'.split(' ')

encode_dict = tokenizer.encode_plus(text=tokens_a,
                  text_pair=tokens_b,
                  max_length=20,
                  pad_to_max_length=True,
                  truncation_strategy='only_second',
                  is_pretokenized=True,
                  return_token_type_ids=True,
                  return_attention_mask=True)
```

å½“ç„¶è¿˜æœ‰batch_encode()å’Œbatch_encode_plus()è¿›è¡Œæ‰¹é‡å¤„ç†çš„ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ä¸åŒç‰ˆæœ¬çš„transformerså¯èƒ½é‡Œé¢çš„å‚æ•°æœ‰ç¨è®¸ä¸åŒã€‚

## è¡¥å……

å½“ç„¶æˆ‘ä»¬ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨tokenizeræ¥å¯¹æ–‡æœ¬è¿›è¡Œå¤„ç†ã€ä¼šè¿”å›attention_maskå’Œtoken_type_idsã€‚

```python
text = ['æˆ‘çˆ±åŒ—äº¬', 'æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨']
raw_tokens = tokenizer(['æˆ‘çˆ±åŒ—äº¬', 'æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨'])
```

## è®­ç»ƒæ–°çš„åˆ†è¯å™¨

```python
from datasets import load_dataset

# åŠ è½½æ•°æ®é›†
data_file = "ChnSentiCorp_htl_all.csv"
dataset = load_dataset("csv", data_files=data_file)
dataset = dataset.filter(lambda x: x["review"] is not None)
dataset = dataset["train"].train_test_split(0.2, seed=123)

from transformers import AutoTokenizer

old_tokenizer = AutoTokenizer.from_pretrained("hfl/chinese-bert-wwm-ext")

def get_training_corpus(dataset):
    dataset = dataset["train"]
    for start_idx in range(0, len(dataset), 1000):
        samples = dataset[start_idx : start_idx + 1000]
        yield samples["review"]

training_corpus = get_training_corpus(dataset)
tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)

tokenizer.save("xxx")
```

## è‹±æ–‡åˆ†è¯å™¨-å‘½åå®ä½“è¯†åˆ«

```python
from transformers import pipeline

token_classifier = pipeline("token-classification")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

é»˜è®¤ä½¿ç”¨çš„æ˜¯ï¼šdbmdz/bert-large-cased-finetuned-conll03-english

```
[{'entity': 'I-PER', 'score': 0.99938285, 'index': 4, 'word': 'S', 'start': 11, 'end': 12}, {'entity': 'I-PER', 'score': 0.99815494, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14}, {'entity': 'I-PER', 'score': 0.99590707, 'index': 6, 'word': '##va', 'start': 14, 'end': 16}, {'entity': 'I-PER', 'score': 0.99923277, 'index': 7, 'word': '##in', 'start': 16, 'end': 18}, {'entity': 'I-ORG', 'score': 0.9738931, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35}, {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40}, {'entity': 'I-ORG', 'score': 0.9887976, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45}, {'entity': 'I-LOC', 'score': 0.9932106, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

æœ‰ç‚¹ç–‘é—®ï¼Œä¸ºä»€ä¹ˆæ‰€æœ‰çš„æ ‡ç­¾éƒ½æ˜¯Iå‘¢ï¼Ÿ

![image-20230427100106028](2.tokenizer.assets\image-20230427100106028.png)

å› ä¸ºBIOæœ‰ä¸¤ç§æ ‡è®°æ ¼å¼ï¼Œè¿™é‡Œæ˜¯ç´«è‰²éƒ¨åˆ†è¡¨ç¤ºçš„ã€‚Båªæ˜¯ç”¨äºåŒºåˆ†ä¸¤ä¸ªè¿ç€çš„ç›¸åŒçš„ç±»çš„ã€‚è€Œä¸Šè¿°é¢„è®­ç»ƒæ¨¡å‹æ­£å¥½æ˜¯ä½¿ç”¨è¿™ç§æ•°æ®è®­ç»ƒçš„ã€‚

æŠŠå®ä½“èšåˆèµ·æ¥ï¼š

```python
from transformers import pipeline

token_classifier = pipeline("token-classification", aggregation_strategy="simple")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

å¯é€‰simpleã€maxã€averageã€first

ç›´æ¥ä½¿ç”¨æ¨¡å‹å¾—åˆ°ç»“æœè€Œä¸æ˜¯ç®¡é“ï¼š

```python
from transformers import AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="pt")
outputs = model(**inputs)

import torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()
predictions = outputs.logits.argmax(dim=-1)[0].tolist()
print(predictions)

results = []
tokens = inputs.tokens()

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        results.append(
            {"entity": label, "score": probabilities[idx][pred], "word": tokens[idx]}
        )

print(results)

"""
[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]
[{'entity': 'I-PER', 'score': 0.9993828535079956, 'word': 'S'}, {'entity': 'I-PER', 'score': 0.9981548190116882, 'word': '##yl'}, {'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va'}, {'entity': 'I-PER', 'score': 0.9992327690124512, 'word': '##in'}, {'entity': 'I-ORG', 'score': 0.9738931059837341, 'word': 'Hu'}, {'entity': 'I-ORG', 'score': 0.9761149883270264, 'word': '##gging'}, {'entity': 'I-ORG', 'score': 0.9887974858283997, 'word': 'Face'}, {'entity': 'I-LOC', 'score': 0.99321049451828, 'word': 'Brooklyn'}]
"""
```

è¿™é‡Œå¾—åˆ°çš„ç´¢å¼•æ˜¯tokenizerä¹‹åæ–‡æœ¬åœ¨tokensé‡Œé¢çš„ç´¢å¼•ï¼Œæ€ä¹ˆå’ŒåŸæ–‡çš„ç´¢å¼•è¿›è¡Œå¯¹åº”å‘¢ï¼Ÿå¯ä»¥ä½¿ç”¨offset_mapping

```python
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
inputs_with_offsets["offset_mapping"]

"""
[(0, 0),
 (0, 2),
 (3, 7),
 (8, 10),
 (11, 12),
 (12, 14),
 (14, 16),
 (16, 18),
 (19, 22),
 (23, 24),
 (25, 29),
 (30, 32),
 (33, 35),
 (35, 40),
 (41, 45),
 (46, 48),
 (49, 57),
 (57, 58),
 (0, 0)]
"""
```

é‡Œé¢(0,0)æ˜¯ç‰¹æ®Šæ ‡è®°ï¼Œæ¯ä¸€ä¸ªå…ƒç¥–æ˜¯è¯¥tokenå¯¹åº”åŸæ–‡çš„å¼€å§‹å’Œç»“å°¾ç´¢å¼•ã€‚

```python
text = "My name is Sylvain and I work at Hugging Face in Brooklyn."
tokens = tokenizer.convert_ids_to_tokens([101, 1422, 1271, 1110, 156, 7777, 2497, 1394, 1105, 146, 1250, 1120, 20164, 10932, 10289, 1107, 6010, 119, 102])
mappings = tokenizer(example, return_offsets_mapping=True)["offset_mapping"]
for t,m in zip(tokens, mappings):
  print(t, m, text[m[0]:m[1]])

"""
[CLS] (0, 0) 
My (0, 2) My
name (3, 7) name
is (8, 10) is
S (11, 12) S
##yl (12, 14) yl
##va (14, 16) va
##in (16, 18) in
and (19, 22) and
I (23, 24) I
work (25, 29) work
at (30, 32) at
Hu (33, 35) Hu
##gging (35, 40) gging
Face (41, 45) Face
in (46, 48) in
Brooklyn (49, 57) Brooklyn
. (57, 58) .
[SEP] (0, 0) 
"""
```

ä¹‹åæˆ‘ä»¬å¯ä»¥è¿™ä¹ˆé‡æ•´ç´¢å¼•ï¼š

```python
results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        start, end = offsets[idx]
        results.append(
            {
                "entity": label,
                "score": probabilities[idx][pred],
                "word": tokens[idx],
                "start": start,
                "end": end,
            }
        )

print(results)
"""
[{'entity': 'I-PER', 'score': 0.9993828535079956, 'word': 'S', 'start': 11, 'end': 12}, {'entity': 'I-PER', 'score': 0.9981548190116882, 'word': '##yl', 'start': 12, 'end': 14}, {'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va', 'start': 14, 'end': 16}, {'entity': 'I-PER', 'score': 0.9992327690124512, 'word': '##in', 'start': 16, 'end': 18}, {'entity': 'I-ORG', 'score': 0.9738931059837341, 'word': 'Hu', 'start': 33, 'end': 35}, {'entity': 'I-ORG', 'score': 0.9761149883270264, 'word': '##gging', 'start': 35, 'end': 40}, {'entity': 'I-ORG', 'score': 0.9887974858283997, 'word': 'Face', 'start': 41, 'end': 45}, {'entity': 'I-LOC', 'score': 0.99321049451828, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
"""
```

èšåˆå®ä½“ï¼š

```python
import numpy as np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

idx = 0
while idx < len(predictions):
    pred = predictions[idx]
    label = model.config.id2label[pred]
    if label != "O":
        # Remove the B- or I-
        label = label[2:]
        start, _ = offsets[idx]

        # Grab all the tokens labeled with I-label
        all_scores = []
        while (
            idx < len(predictions)
            and model.config.id2label[predictions[idx]] == f"I-{label}"
        ):
            all_scores.append(probabilities[idx][pred])
            _, end = offsets[idx]
            idx += 1

        # The score is the mean of all the scores of the tokens in that grouped entity
        score = np.mean(all_scores).item()
        word = example[start:end]
        results.append(
            {
                "entity_group": label,
                "score": score,
                "word": word,
                "start": start,
                "end": end,
            }
        )
    idx += 1

print(results)
```

## è‹±æ–‡åˆ†è¯å™¨-é—®ç­”

```python
from transformers import pipeline
question_answerer = pipeline("question-answering")

long_context = """
ğŸ¤— Transformers: State of the Art NLP

ğŸ¤— Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

ğŸ¤— Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internals are exposed as consistently as possible.
  - Model files can be used independently of the library for quick experiments.

ğŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question = "Which deep learning libraries back ğŸ¤— Transformers?"
question_answerer(question=question, context=long_context)

"""
No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).
Using a pipeline without specifying a model name and revision in production is not recommended.
{'score': 0.9714871048927307,
 'start': 1892,
 'end': 1919,
 'answer': 'Jax, PyTorch and TensorFlow'}
"""
```

å®ƒæ˜¯æ€ä¹ˆåšåˆ°èƒ½å¤Ÿå›ç­”è¿™ä¹ˆé•¿çš„æ–‡æ¡£çš„ä¸”ç´¢å¼•ä½ç½®è¿˜æ˜¯å¯¹çš„ï¼Ÿ

æˆ‘ä»¬æŠŠé—®é¢˜å’Œç­”æ¡ˆæˆå¯¹è¾“å…¥åˆ°tokenizerä¸­ï¼š

![image-20230427101723165](2.tokenizer.assets\image-20230427101723165.png)

```python
from transformers import AutoTokenizer, AutoModelForQuestionAnswering

model_checkpoint = "distilbert-base-cased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

context = long_context
inputs = tokenizer(question, context, return_tensors="pt")
outputs = model(**inputs)
start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape)
```

ä¸ºäº†å°†è¿™äº›å¯¹æ•°è½¬æ¢æˆæ¦‚ç‡ï¼Œæˆ‘ä»¬å°†åº”ç”¨ä¸€ä¸ªsoftmaxå‡½æ•°--ä½†åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿å±è”½ä¸å±äºä¸Šä¸‹æ–‡çš„ç´¢å¼•ã€‚æˆ‘ä»¬çš„è¾“å…¥æ˜¯[CLS]é—®é¢˜[SEP]ä¸Šä¸‹æ–‡[SEP]ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å±è”½é—®é¢˜çš„æ ‡è®°ä»¥åŠ[SEP]æ ‡è®°ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å°†ä¿ç•™[CLS]æ ‡è®°ï¼Œå› ä¸ºæœ‰äº›æ¨¡å‹ç”¨å®ƒæ¥è¡¨ç¤ºç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚

ç”±äºæˆ‘ä»¬å°†åœ¨ä¹‹ååº”ç”¨softmaxï¼Œæˆ‘ä»¬åªéœ€è¦ç”¨ä¸€ä¸ªå¤§çš„è´Ÿæ•°æ¥æ›¿æ¢æˆ‘ä»¬æƒ³è¦å±è”½çš„å¯¹æ•°ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨-10000ï¼š

```python
import torch

sequence_ids = inputs.sequence_ids()
print(sequence_ids)
# Mask everything apart from the tokens of the context
mask = [i != 1 for i in sequence_ids]
# Unmask the [CLS] token
mask[0] = False
mask = torch.tensor(mask)[None]

start_logits[mask] = -10000
end_logits[mask] = -10000

start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]
```

ç­”æ¡ˆä»start_indexå¼€å§‹åˆ°end_indexç»“æŸçš„æ¦‚ç‡æ˜¯ï¼šstart_probabilities[start_index]Ã—end_probabilities[end_index]

```python
scores = start_probabilities[:, None] * end_probabilities[None, :]
```

ç„¶åæˆ‘ä»¬å°†æŠŠstart_index>end_indexçš„å€¼è®¾ç½®ä¸º0æ¥å±è”½ï¼ˆå…¶ä»–æ¦‚ç‡éƒ½æ˜¯æ­£æ•°ï¼‰ã€‚torch.triu()å‡½æ•°è¿”å›ä½œä¸ºå‚æ•°ä¼ é€’çš„äºŒç»´å¼ é‡çš„ä¸Šä¸‰è§’éƒ¨åˆ†ï¼Œæ‰€ä»¥å®ƒå°†ä¸ºæˆ‘ä»¬åšè¿™ä¸ªå±è”½ï¼š

```python
scores = torch.triu(scores)
```

ç°åœ¨æˆ‘ä»¬åªéœ€è¦å¾—åˆ°æœ€å¤§å€¼çš„ç´¢å¼•ã€‚ç”±äºPyTorchå°†è¿”å›æ‰å¹³åŒ–å¼ é‡ä¸­çš„ç´¢å¼•ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨åœ°æ¿é™¤æ³•//å’Œæ¨¡æ•°%æ“ä½œæ¥è·å¾—start_indexå’Œend_indexï¼š

```python
max_index = scores.argmax().item()
start_index = max_index // scores.shape[1]
end_index = max_index % scores.shape[1]
print(scores[start_index, end_index])
```

æˆ‘ä»¬è¿˜æ²¡æœ‰å®Œå…¨å®Œæˆï¼Œä½†è‡³å°‘æˆ‘ä»¬å·²ç»æœ‰äº†ç­”æ¡ˆçš„æ­£ç¡®åˆ†æ•°ï¼ˆä½ å¯ä»¥é€šè¿‡ä¸ä¸Šä¸€èŠ‚çš„ç¬¬ä¸€ä¸ªç»“æœç›¸æ¯”è¾ƒæ¥æ£€æŸ¥ï¼‰ã€‚

æˆ‘ä»¬æœ‰ç­”æ¡ˆçš„start_indexå’Œend_indexï¼Œæ‰€ä»¥ç°åœ¨æˆ‘ä»¬åªéœ€è¦è½¬æ¢ä¸ºä¸Šä¸‹æ–‡ä¸­çš„å­—ç¬¦ç´¢å¼•ã€‚è¿™æ—¶ï¼Œåç§»é‡å°±ä¼šå˜å¾—éå¸¸æœ‰ç”¨ã€‚æˆ‘ä»¬å¯ä»¥åƒåœ¨æ ‡è®°åˆ†ç±»ä»»åŠ¡ä¸­é‚£æ ·æŠ“å–å¹¶ä½¿ç”¨å®ƒä»¬ï¼š

```python
inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)
offsets = inputs_with_offsets["offset_mapping"]

start_char, _ = offsets[start_index]
_, end_char = offsets[end_index]
answer = context[start_char:end_char]
```

ç°åœ¨æˆ‘ä»¬åªéœ€å¯¹æ‰€æœ‰çš„ä¸œè¥¿è¿›è¡Œæ ¼å¼åŒ–ï¼Œå°±å¯ä»¥å¾—åˆ°æˆ‘ä»¬çš„ç»“æœï¼š

```python
result = {
    "answer": answer,
    "start": start_char,
    "end": end_char,
    "score": scores[start_index, end_index],
}
print(result)
```

### æ€ä¹ˆå¤„ç†é•¿æ–‡æœ¬ï¼Ÿ

å¦‚æœæˆ‘ä»¬è¯•å›¾å¯¹æˆ‘ä»¬ä¹‹å‰ç”¨ä½œä¾‹å­çš„é—®é¢˜å’Œé•¿ä¸Šä¸‹æ–‡è¿›è¡Œæ ‡è®°ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ä¸ªé«˜äºé—®é¢˜-å›ç­”ç®¡é“ä¸­ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ï¼ˆå³384ï¼‰çš„æ ‡è®°æ•°ï¼š

```python
inputs = tokenizer(question, long_context)
print(len(inputs["input_ids"]))
```

å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦åœ¨è¿™ä¸ªæœ€å¤§é•¿åº¦ä¸Šæˆªæ–­æˆ‘ä»¬çš„è¾“å…¥ã€‚æˆ‘ä»¬æœ‰å‡ ç§æ–¹æ³•å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œä½†æˆ‘ä»¬ä¸æƒ³æˆªæ–­é—®é¢˜ï¼Œåªæƒ³æˆªæ–­ä¸Šä¸‹æ–‡ã€‚ç”±äºä¸Šä¸‹æ–‡æ˜¯ç¬¬äºŒå¥è¯ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ "only_second "çš„æˆªæ–­ç­–ç•¥ã€‚è¿™æ—¶å‡ºç°çš„é—®é¢˜æ˜¯ï¼Œé—®é¢˜çš„ç­”æ¡ˆå¯èƒ½ä¸åœ¨è¢«æˆªæ–­çš„ä¸Šä¸‹æ–‡ä¸­ã€‚ä¾‹å¦‚ï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æŒ‘é€‰äº†ä¸€ä¸ªç­”æ¡ˆåœ¨ä¸Šä¸‹æ–‡æœ«å°¾çš„é—®é¢˜ï¼Œè€Œå½“æˆ‘ä»¬æˆªæ–­å®ƒæ—¶ï¼Œè¿™ä¸ªç­”æ¡ˆå°±ä¸å­˜åœ¨äº†ï¼š

```python
inputs = tokenizer(question, long_context, max_length=384, truncation="only_second")
print(tokenizer.decode(inputs["input_ids"]))
```

è¿™æ„å‘³ç€æ¨¡å‹å°†å¾ˆéš¾æŒ‘é€‰å‡ºæ­£ç¡®çš„ç­”æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå›ç­”é—®é¢˜çš„ç®¡é“å…è®¸æˆ‘ä»¬å°†ä¸Šä¸‹æ–‡åˆ†å‰²æˆè¾ƒå°çš„å—ï¼ŒæŒ‡å®šæœ€å¤§é•¿åº¦ã€‚ä¸ºäº†ç¡®ä¿æˆ‘ä»¬ä¸ä¼šåœ¨å®Œå…¨é”™è¯¯çš„åœ°æ–¹åˆ†å‰²ä¸Šä¸‹æ–‡ï¼Œä½¿å…¶æœ‰å¯èƒ½æ‰¾åˆ°ç­”æ¡ˆï¼Œå®ƒè¿˜åŒ…æ‹¬å„å—ä¹‹é—´çš„ä¸€äº›é‡å ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡æ·»åŠ return_overflowing_tokens=Trueæ¥è®©æ ‡è®°å™¨ï¼ˆå¿«æˆ–æ…¢ï¼‰ä¸ºæˆ‘ä»¬åšè¿™ä»¶äº‹ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨strideå‚æ•°æŒ‡å®šæˆ‘ä»¬æƒ³è¦çš„é‡å ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ï¼Œä½¿ç”¨ä¸€ä¸ªè¾ƒå°çš„å¥å­ï¼š

```python
sentence = "This sentence is not too long but we are going to split it anyway."
inputs = tokenizer(
    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

for ids in inputs["input_ids"]:
    print(tokenizer.decode(ids))
```

æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œè¯¥å¥å­è¢«åˆ†å‰²æˆè‹¥å¹²å—ï¼Œåœ¨inputs["input_ids"]ä¸­çš„æ¯ä¸ªæ¡ç›®æœ€å¤šåªæœ‰6ä¸ªæ ‡è®°ï¼ˆæˆ‘ä»¬éœ€è¦æ·»åŠ å¡«å……ç‰©ä»¥ä½¿æœ€åä¸€ä¸ªæ¡ç›®ä¸å…¶ä»–æ¡ç›®çš„å¤§å°ç›¸åŒï¼‰ï¼Œæ¯ä¸ªæ¡ç›®ä¹‹é—´æœ‰2ä¸ªæ ‡è®°çš„é‡å ã€‚

è®©æˆ‘ä»¬ä»”ç»†çœ‹ä¸€ä¸‹æ ‡è®°åŒ–çš„ç»“æœï¼š

æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œæˆ‘ä»¬å¾—åˆ°äº†è¾“å…¥IDå’Œä¸€ä¸ªæ³¨æ„åŠ›æ©ç ã€‚æœ€åä¸€ä¸ªé”®ï¼Œoverflow_to_sample_mappingï¼Œæ˜¯ä¸€ä¸ªåœ°å›¾ï¼Œå‘Šè¯‰æˆ‘ä»¬æ¯ä¸ªç»“æœå¯¹åº”çš„å¥å­--è¿™é‡Œæˆ‘ä»¬æœ‰7ä¸ªç»“æœï¼Œéƒ½æ¥è‡ªæˆ‘ä»¬ä¼ é€’ç»™æ ‡è®°å™¨çš„ï¼ˆå”¯ä¸€ï¼‰å¥å­ï¼š

å½“æˆ‘ä»¬å¯¹å‡ ä¸ªå¥å­ä¸€èµ·è¿›è¡Œæ ‡è®°æ—¶ï¼Œè¿™å°±æ›´æœ‰ç”¨äº†ã€‚ä¾‹å¦‚ï¼Œè¿™ä¸ªï¼š

```python
sentences = [
    "This sentence is not too long but we are going to split it anyway.",
    "This sentence is shorter but will still get split.",
]
inputs = tokenizer(
    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

print(inputs["overflow_to_sample_mapping"])

"""
[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]
"""
```

è¿™æ„å‘³ç€ç¬¬ä¸€å¥è¯åƒä»¥å‰ä¸€æ ·è¢«åˆ†æˆäº†7å—ï¼Œæ¥ä¸‹æ¥çš„4å—æ¥è‡ªç¬¬äºŒå¥ã€‚

ç°åœ¨è®©æˆ‘ä»¬å›åˆ°æˆ‘ä»¬çš„é•¿ä¸Šä¸‹æ–‡ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œé—®é¢˜å›ç­”ç®¡é“ä½¿ç”¨çš„æœ€å¤§é•¿åº¦æ˜¯384ï¼Œæ­£å¦‚æˆ‘ä»¬å‰é¢æåˆ°çš„ï¼Œstrideæ˜¯128ï¼Œè¿™ä¸æ¨¡å‹çš„å¾®è°ƒæ–¹å¼ç›¸å¯¹åº”ï¼ˆä½ å¯ä»¥åœ¨è°ƒç”¨ç®¡é“æ—¶é€šè¿‡ä¼ é€’max_seq_lenå’Œstrideå‚æ•°æ¥è°ƒæ•´è¿™äº›å‚æ•°ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†åœ¨æ ‡è®°åŒ–æ—¶ä½¿ç”¨è¿™äº›å‚æ•°ã€‚æˆ‘ä»¬è¿˜å°†æ·»åŠ å¡«å……ï¼ˆä¸ºäº†æœ‰ç›¸åŒé•¿åº¦çš„æ ·æœ¬ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å»ºç«‹å¼ é‡ï¼‰ä»¥åŠè¯¢é—®åç§»é‡ï¼š

```python
inputs = tokenizer(
    question,
    long_context,
    stride=128,
    max_length=384,
    padding="longest",
    truncation="only_second",
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)
```

è¿™äº›è¾“å…¥å°†åŒ…å«æ¨¡å‹æ‰€æœŸæœ›çš„è¾“å…¥IDå’Œæ³¨æ„æ©ç ï¼Œä»¥åŠæˆ‘ä»¬åˆšæ‰è°ˆåˆ°çš„åç§»é‡å’Œæº¢å‡º_åˆ°_æ ·æœ¬_æ˜ å°„ã€‚ç”±äºè¿™ä¸¤ä¸ªä¸æ˜¯æ¨¡å‹ä½¿ç”¨çš„å‚æ•°ï¼Œæˆ‘ä»¬åœ¨å°†å…¶è½¬æ¢ä¸ºå¼ é‡ä¹‹å‰ï¼Œä¼šå°†å®ƒä»¬ä»è¾“å…¥ä¸­å¼¹å‡ºï¼ˆæˆ‘ä»¬ä¸ä¼šå­˜å‚¨åœ°å›¾ï¼Œå› ä¸ºå®ƒåœ¨è¿™é‡Œæ²¡æœ‰ç”¨ï¼‰ï¼š

```python
_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("pt")
print(inputs["input_ids"].shape)
```

æˆ‘ä»¬çš„é•¿æ–‡è¢«ä¸€åˆ†ä¸ºäºŒï¼Œè¿™æ„å‘³ç€åœ¨å®ƒé€šè¿‡æˆ‘ä»¬çš„æ¨¡å‹åï¼Œæˆ‘ä»¬å°†æœ‰ä¸¤å¥—å¼€å§‹å’Œç»“æŸçš„å¯¹æ•°ï¼š

```python
outputs = model(**inputs)

start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape)
```

åƒä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬é¦–å…ˆæ©ç›–ä¸å±äºä¸Šä¸‹æ–‡çš„æ ‡è®°ï¼Œç„¶åå†è¿›è¡Œsoftmaxè®¡ç®—ã€‚æˆ‘ä»¬è¿˜å±è”½äº†æ‰€æœ‰çš„å¡«å……æ ‡è®°ï¼ˆå¦‚æ³¨æ„åŠ›å±è”½æ‰€æ ‡å¿—çš„ï¼‰ï¼š

```python
sequence_ids = inputs.sequence_ids()
# Mask everything apart from the tokens of the context
mask = [i != 1 for i in sequence_ids]
# Unmask the [CLS] token
mask[0] = False
# Mask all the [PAD] tokens
mask = torch.logical_or(torch.tensor(mask)[None], (inputs["attention_mask"] == 0))

start_logits[mask] = -10000
end_logits[mask] = -10000

start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)

```

ä¸‹ä¸€æ­¥ç±»ä¼¼äºæˆ‘ä»¬å¯¹å°èƒŒæ™¯æ‰€åšçš„å·¥ä½œï¼Œä½†æˆ‘ä»¬å¯¹æˆ‘ä»¬çš„ä¸¤ä¸ªå—ä¸­çš„æ¯ä¸€ä¸ªéƒ½è¿›è¡Œé‡å¤ã€‚æˆ‘ä»¬ç»™æ‰€æœ‰å¯èƒ½çš„ç­”æ¡ˆè·¨åº¦æ‰“åˆ†ï¼Œç„¶åé€‰æ‹©å¾—åˆ†æœ€é«˜çš„è·¨åº¦ï¼š

```pyth
candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = torch.triu(scores).argmax().item()

    start_idx = idx // scores.shape[1]
    end_idx = idx % scores.shape[1]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)
```

è¿™ä¸¤ä¸ªå€™é€‰è€…å¯¹åº”äºæ¨¡å‹åœ¨æ¯å—å†…å®¹ä¸­èƒ½å¤Ÿæ‰¾åˆ°çš„æœ€ä½³ç­”æ¡ˆã€‚æ¨¡å‹å¯¹æ­£ç¡®ç­”æ¡ˆåœ¨ç¬¬äºŒéƒ¨åˆ†æ›´æœ‰ä¿¡å¿ƒï¼ˆè¿™æ˜¯ä¸ªå¥½è¿¹è±¡ï¼ï¼‰ã€‚ç°åœ¨æˆ‘ä»¬åªéœ€è¦å°†è¿™ä¸¤ä¸ªæ ‡è®°çš„è·¨åº¦æ˜ å°„åˆ°ä¸Šä¸‹æ–‡ä¸­çš„å­—ç¬¦è·¨åº¦ï¼ˆæˆ‘ä»¬åªéœ€è¦æ˜ å°„ç¬¬äºŒä¸ªæ ‡è®°å°±å¯ä»¥å¾—åˆ°æˆ‘ä»¬çš„ç­”æ¡ˆï¼Œä½†æ˜¯çœ‹çœ‹æ¨¡å‹åœ¨ç¬¬ä¸€å—ä¸­é€‰æ‹©äº†ä»€ä¹ˆå¾ˆæœ‰æ„æ€ï¼‰ã€‚

æˆ‘ä»¬ä¹‹å‰æŠ“å–çš„åç§»é‡å®é™…ä¸Šæ˜¯ä¸€ä¸ªåç§»é‡çš„åˆ—è¡¨ï¼Œæ¯å—æ–‡æœ¬æœ‰ä¸€ä¸ªåˆ—è¡¨ï¼š

```python
for candidate, offset in zip(candidates, offsets):
    start_token, end_token, score = candidate
    start_char, _ = offset[start_token]
    _, end_char = offset[end_token]
    answer = long_context[start_char:end_char]
    result = {"answer": answer, "start": start_char, "end": end_char, "score": score}
    print(result)
```

## å­—èŠ‚å¯¹ç¼–ç 

![The tokenization pipeline.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg)

åœ¨å°†ä¸€ä¸ªæ–‡æœ¬åˆ†å‰²æˆå­ç¬¦å·ï¼ˆæ ¹æ®å…¶æ¨¡å‹ï¼‰ä¹‹å‰ï¼Œæ ‡è®°å™¨æ‰§è¡Œä¸¤ä¸ªæ­¥éª¤ï¼šè§„èŒƒåŒ–å’Œé¢„æ ‡è®°ã€‚

ğŸ¤— Transformersæ ‡è®°å™¨æœ‰ä¸€ä¸ªåä¸ºbackend_tokenizerçš„å±æ€§ï¼Œå®ƒæä¾›äº†å¯¹æ¥è‡ªğŸ¤—Tokenizersåº“çš„åº•å±‚æ ‡è®°å™¨çš„è®¿é—®ï¼š

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
print(type(tokenizer.backend_tokenizer))
```

tokenizerå¯¹è±¡çš„normalizerå±æ€§æœ‰ä¸€ä¸ªnormalize_str()æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥æŸ¥çœ‹å¦‚ä½•è¿›è¡Œè§„èŒƒåŒ–å¤„ç†ï¼š

```python
print(tokenizer.backend_tokenizer.normalizer.normalize_str("HÃ©llÃ² hÃ´w are Ã¼?"))
```

**å­—èŠ‚å¯¹ç¼–ç **

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")

from collections import defaultdict

word_freqs = defaultdict(int)

# é¢„æ ‡è®°åŒ–ï¼Œå¹¶ç»Ÿè®¡è¯é¢‘
for text in corpus:
    #Ä è¡¨ç¤ºç©ºæ ¼çš„æ„æ€
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    print(words_with_offsets)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

print(word_freqs)
"""
[('This', (0, 4)), ('Ä is', (4, 7)), ('Ä the', (7, 11)), ('Ä Hugging', (11, 19)), ('Ä Face', (19, 24)), ('Ä Course', (24, 31)), ('.', (31, 32))]
[('This', (0, 4)), ('Ä chapter', (4, 12)), ('Ä is', (12, 15)), ('Ä about', (15, 21)), ('Ä tokenization', (21, 34)), ('.', (34, 35))]
[('This', (0, 4)), ('Ä section', (4, 12)), ('Ä shows', (12, 18)), ('Ä several', (18, 26)), ('Ä tokenizer', (26, 36)), ('Ä algorithms', (36, 47)), ('.', (47, 48))]
[('Hopefully', (0, 9)), (',', (9, 10)), ('Ä you', (10, 14)), ('Ä will', (14, 19)), ('Ä be', (19, 22)), ('Ä able', (22, 27)), ('Ä to', (27, 30)), ('Ä understand', (30, 41)), ('Ä how', (41, 45)), ('Ä they', (45, 50)), ('Ä are', (50, 54)), ('Ä trained', (54, 62)), ('Ä and', (62, 66)), ('Ä generate', (66, 75)), ('Ä tokens', (75, 82)), ('.', (82, 83))]
defaultdict(<class 'int'>, {'This': 3, 'Ä is': 2, 'Ä the': 1, 'Ä Hugging': 1, 'Ä Face': 1, 'Ä Course': 1, '.': 4, 'Ä chapter': 1, 'Ä about': 1, 'Ä tokenization': 1, 'Ä section': 1, 'Ä shows': 1, 'Ä several': 1, 'Ä tokenizer': 1, 'Ä algorithms': 1, 'Hopefully': 1, ',': 1, 'Ä you': 1, 'Ä will': 1, 'Ä be': 1, 'Ä able': 1, 'Ä to': 1, 'Ä understand': 1, 'Ä how': 1, 'Ä they': 1, 'Ä are': 1, 'Ä trained': 1, 'Ä and': 1, 'Ä generate': 1, 'Ä tokens': 1})
"""

# æ‹†åˆ†å¾—åˆ°å­—ç¬¦
alphabet = []

for word in word_freqs.keys():
    for letter in word:
        if letter not in alphabet:
            alphabet.append(letter)
alphabet.sort()

print(alphabet)
"""
[',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ä ']
"""
vocab = ["<|endoftext|>"] + alphabet.copy()

# å¯¹æ¯ä¸€ä¸ªè¯æ‹†åˆ†ä¸ºå­—ç¬¦çš„ç»„åˆ
splits = {word: [c for c in word] for word in word_freqs.keys()}
"""
{'This': ['T', 'h', 'i', 's'],
 'Ä is': ['Ä ', 'i', 's'],
 'Ä the': ['Ä ', 't', 'h', 'e'],
 'Ä Hugging': ['Ä ', 'H', 'u', 'g', 'g', 'i', 'n', 'g'],
 'Ä Face': ['Ä ', 'F', 'a', 'c', 'e'],
 'Ä Course': ['Ä ', 'C', 'o', 'u', 'r', 's', 'e'],
 '.': ['.'],
 'Ä chapter': ['Ä ', 'c', 'h', 'a', 'p', 't', 'e', 'r'],
 'Ä about': ['Ä ', 'a', 'b', 'o', 'u', 't'],
 'Ä tokenization': ['Ä ',
  't',
  'o',
  'k',
  'e',
  'n',
  'i',
  'z',
  'a',
  't',
  'i',
  'o',
  'n'],
 'Ä section': ['Ä ', 's', 'e', 'c', 't', 'i', 'o', 'n'],
 'Ä shows': ['Ä ', 's', 'h', 'o', 'w', 's'],
 'Ä several': ['Ä ', 's', 'e', 'v', 'e', 'r', 'a', 'l'],
 'Ä tokenizer': ['Ä ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r'],
 'Ä algorithms': ['Ä ', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's'],
 'Hopefully': ['H', 'o', 'p', 'e', 'f', 'u', 'l', 'l', 'y'],
 ',': [','],
 'Ä you': ['Ä ', 'y', 'o', 'u'],
 'Ä will': ['Ä ', 'w', 'i', 'l', 'l'],
 'Ä be': ['Ä ', 'b', 'e'],
 'Ä able': ['Ä ', 'a', 'b', 'l', 'e'],
 'Ä to': ['Ä ', 't', 'o'],
 'Ä understand': ['Ä ', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd'],
 'Ä how': ['Ä ', 'h', 'o', 'w'],
 'Ä they': ['Ä ', 't', 'h', 'e', 'y'],
 'Ä are': ['Ä ', 'a', 'r', 'e'],
 'Ä trained': ['Ä ', 't', 'r', 'a', 'i', 'n', 'e', 'd'],
 'Ä and': ['Ä ', 'a', 'n', 'd'],
 'Ä generate': ['Ä ', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'e'],
 'Ä tokens': ['Ä ', 't', 'o', 'k', 'e', 'n', 's']}
"""

# è®¡ç®—ä¸¤ä¸¤å­—ç¬¦ç»„åˆå‡ºç°çš„é¢‘ç‡
def compute_pair_freqs(splits):
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            pair_freqs[pair] += freq
    return pair_freqs

pair_freqs = compute_pair_freqs(splits)

for i, key in enumerate(pair_freqs.keys()):
    print(f"{key}: {pair_freqs[key]}")
    if i >= 5:
        break

"""
('T', 'h'): 3
('h', 'i'): 3
('i', 's'): 5
('Ä ', 'i'): 2
('Ä ', 't'): 7
('t', 'h'): 3
"""

# å–å‡ºé¢‘ç‡æœ€é«˜çš„
best_pair = ""
max_freq = None

for pair, freq in pair_freqs.items():
    if max_freq is None or max_freq < freq:
        best_pair = pair
        max_freq = freq

print(best_pair, max_freq)

# åˆå¹¶å¹¶æ·»åŠ åˆ°vocabé‡Œé¢
merges = {("Ä ", "t"): "Ä t"}
vocab.append("Ä t")

# å†™æˆå‡½æ•°
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue

        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                split = split[:i] + [a + b] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits

splits = merge_pair("Ä ", "t", splits)
print(splits["Ä trained"])

"""
['Ä t', 'r', 'a', 'i', 'n', 'e', 'd']
"""

vocab_size = 50

while len(vocab) < vocab_size:
    pair_freqs = compute_pair_freqs(splits)
    best_pair = ""
    max_freq = None
    for pair, freq in pair_freqs.items():
        if max_freq is None or max_freq < freq:
            best_pair = pair
            max_freq = freq
    splits = merge_pair(*best_pair, splits)
    merges[best_pair] = best_pair[0] + best_pair[1]
    vocab.append(best_pair[0] + best_pair[1])

print(merges)
"""
('Ä ', 't'): 'Ä t', ('i', 's'): 'is', ('e', 'r'): 'er', ('Ä ', 'a'): 'Ä a', ('Ä t', 'o'): 'Ä to', ('e', 'n'): 'en', ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('Ä to', 'k'): 'Ä tok', ('Ä tok', 'en'): 'Ä token', ('n', 'd'): 'nd', ('Ä ', 'is'): 'Ä is', ('Ä t', 'h'): 'Ä th', ('Ä th', 'e'): 'Ä the', ('i', 'n'): 'in', ('Ä a', 'b'): 'Ä ab', ('Ä token', 'i'): 'Ä tokeni'}
"""
print(vocab)
"""
['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o',
 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'Ä ', 'Ä t', 'is', 'er', 'Ä a', 'Ä to', 'en', 'Th', 'This', 'ou', 'se',
 'Ä tok', 'Ä token', 'nd', 'Ä is', 'Ä th', 'Ä the', 'in', 'Ä ab', 'Ä tokeni']
"""

# ä½¿ç”¨
def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    splits = [[l for l in word] for word in pre_tokenized_text]
    for pair, merge in merges.items():
        for idx, split in enumerate(splits):
            i = 0
            while i < len(split) - 1:
                if split[i] == pair[0] and split[i + 1] == pair[1]:
                    split = split[:i] + [merge] + split[i + 2 :]
                else:
                    i += 1
            splits[idx] = split

    return sum(splits, [])


tokenize("This is not a token.")

"""
['This', 'Ä is', 'Ä ', 'n', 'o', 't', 'Ä a', 'Ä token', '.']
"""
```

## WordPieceç¼–ç 

ä½¿ç”¨åˆ†æ•°è¿›è¡Œé…å¯¹ï¼š

![image-20230427111254717](C:\Users\Administrator\Desktop\github\python_common_code_collection\src\huggingface\2.tokenizer.assets\image-20230427111254717.png)

è¯¥ç®—æ³•ä¼˜å…ˆåˆå¹¶é‚£äº›å•ä¸ªéƒ¨åˆ†åœ¨è¯æ±‡ä¸­å‡ºç°é¢‘ç‡è¾ƒä½çš„é…å¯¹ã€‚

```python
corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs

"""
defaultdict(int,
            {'This': 3,
             'is': 2,
             'the': 1,
             'Hugging': 1,
             'Face': 1,
             'Course': 1,
             '.': 4,
             'chapter': 1,
             'about': 1,
             'tokenization': 1,
             'section': 1,
             'shows': 1,
             'several': 1,
             'tokenizer': 1,
             'algorithms': 1,
             'Hopefully': 1,
             ',': 1,
             'you': 1,
             'will': 1,
             'be': 1,
             'able': 1,
             'to': 1,
             'understand': 1,
             'how': 1,
             'they': 1,
             'are': 1,
             'trained': 1,
             'and': 1,
             'generate': 1,
             'tokens': 1})
"""
alphabet = []
for word in word_freqs.keys():
    if word[0] not in alphabet:
        alphabet.append(word[0])
    for letter in word[1:]:
        if f"##{letter}" not in alphabet:
            alphabet.append(f"##{letter}")

alphabet.sort()

print(alphabet)

vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"] + alphabet.copy()
splits = {
    word: [c if i == 0 else f"##{c}" for i, c in enumerate(word)]
    for word in word_freqs.keys()
}
splits

"""
{'This': ['T', '##h', '##i', '##s'],
 'is': ['i', '##s'],
 'the': ['t', '##h', '##e'],
 'Hugging': ['H', '##u', '##g', '##g', '##i', '##n', '##g'],
 'Face': ['F', '##a', '##c', '##e'],
 'Course': ['C', '##o', '##u', '##r', '##s', '##e'],
 '.': ['.'],
 'chapter': ['c', '##h', '##a', '##p', '##t', '##e', '##r'],
 'about': ['a', '##b', '##o', '##u', '##t'],
 'tokenization': ['t',
  '##o',
  '##k',
  '##e',
  '##n',
  '##i',
  '##z',
  '##a',
  '##t',
  '##i',
  '##o',
  '##n'],
 'section': ['s', '##e', '##c', '##t', '##i', '##o', '##n'],
 'shows': ['s', '##h', '##o', '##w', '##s'],
 'several': ['s', '##e', '##v', '##e', '##r', '##a', '##l'],
 'tokenizer': ['t', '##o', '##k', '##e', '##n', '##i', '##z', '##e', '##r'],
 'algorithms': ['a',
  '##l',
  '##g',
  '##o',
  '##r',
  '##i',
  '##t',
  '##h',
  '##m',
  '##s'],
 'Hopefully': ['H', '##o', '##p', '##e', '##f', '##u', '##l', '##l', '##y'],
 ',': [','],
 'you': ['y', '##o', '##u'],
 'will': ['w', '##i', '##l', '##l'],
 'be': ['b', '##e'],
 'able': ['a', '##b', '##l', '##e'],
 'to': ['t', '##o'],
 'understand': ['u',
  '##n',
  '##d',
  '##e',
  '##r',
  '##s',
  '##t',
  '##a',
  '##n',
  '##d'],
 'how': ['h', '##o', '##w'],
 'they': ['t', '##h', '##e', '##y'],
 'are': ['a', '##r', '##e'],
 'trained': ['t', '##r', '##a', '##i', '##n', '##e', '##d'],
 'and': ['a', '##n', '##d'],
 'generate': ['g', '##e', '##n', '##e', '##r', '##a', '##t', '##e'],
 'tokens': ['t', '##o', '##k', '##e', '##n', '##s']}
"""
def compute_pair_scores(splits):
    letter_freqs = defaultdict(int)
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            letter_freqs[split[0]] += freq
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            letter_freqs[split[i]] += freq
            pair_freqs[pair] += freq
        letter_freqs[split[-1]] += freq

    scores = {
        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
        for pair, freq in pair_freqs.items()
    }
    return scores

pair_scores = compute_pair_scores(splits)
for i, key in enumerate(pair_scores.keys()):
    print(f"{key}: {pair_scores[key]}")
    if i >= 5:
        break

"""
('T', '##h'): 0.125
('##h', '##i'): 0.03409090909090909
('##i', '##s'): 0.02727272727272727
('i', '##s'): 0.1
('t', '##h'): 0.03571428571428571
('##h', '##e'): 0.011904761904761904
"""

best_pair = ""
max_score = None
for pair, score in pair_scores.items():
    if max_score is None or max_score < score:
        best_pair = pair
        max_score = score

print(best_pair, max_score)

vocab.append("ab")
def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue
        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                merge = a + b[2:] if b.startswith("##") else a + b
                split = split[:i] + [merge] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits

splits = merge_pair("a", "##b", splaits)
splits["about"]

vocab_size = 70
while len(vocab) < vocab_size:
    scores = compute_pair_scores(splits)
    best_pair, max_score = "", None
    for pair, score in scores.items():
        if max_score is None or max_score < score:
            best_pair = pair
            max_score = score
    splits = merge_pair(*best_pair, splits)
    new_token = (
        best_pair[0] + best_pair[1][2:]
        if best_pair[1].startswith("##")
        else best_pair[0] + best_pair[1]
    )
    vocab.append(new_token)
print(vocab)

"""
['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat', '##ut']
"""
def encode_word(word):
    tokens = []
    while len(word) > 0:
        i = len(word)
        while i > 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return ["[UNK]"]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) > 0:
            word = f"##{word}"
    return tokens
print(encode_word("Hugging"))
print(encode_word("HOgging"))

def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    encoded_words = [encode_word(word) for word in pre_tokenized_text]
    return sum(encoded_words, [])
    
tokenize("This is the Hugging Face course!")
"""
['Th',
 '##i',
 '##s',
 'is',
 'th',
 '##e',
 'Hugg',
 '##i',
 '##n',
 '##g',
 'Fac',
 '##e',
 'c',
 '##o',
 '##u',
 '##r',
 '##s',
 '##e',
 '[UNK]']
"""
```

## Unigramç¼–ç 

[Unigram tokenization - Hugging Face Course](https://huggingface.co/learn/nlp-course/chapter6/7?fw=pt)

## æ›´ç®€ä¾¿çš„è®­ç»ƒtokenizer

More precisely, the library is built around a central `Tokenizer` class with the building blocks regrouped in submodules:

- `normalizers` contains all the possible types of `Normalizer` you can use (complete list [here](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.normalizers)).
- `pre_tokenizers` contains all the possible types of `PreTokenizer` you can use (complete list [here](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.pre_tokenizers)).
- `models` contains the various types of `Model` you can use, like `BPE`, `WordPiece`, and `Unigram` (complete list [here](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.models)).
- `trainers` contains all the different types of `Trainer` you can use to train your model on a corpus (one per type of model; complete list [here](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.trainers)).
- `post_processors` contains the various types of `PostProcessor` you can use (complete list [here](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.processors)).
- `decoders` contains the various types of `Decoder` you can use to decode the outputs of tokenization (complete list [here](https://huggingface.co/docs/tokenizers/python/latest/components.html#decoders)).

You can find the whole list of building blocks [here](https://huggingface.co/docs/tokenizers/python/latest/components.html).

### è·å–è¯­æ–™

```python
from datasets import load_dataset

dataset = load_dataset("wikitext", name="wikitext-2-raw-v1", split="train")


def get_training_corpus():
    for i in range(0, len(dataset), 1000):
        yield dataset[i : i + 1000]["text"]
```

## æ„å»ºå­—èŠ‚å¯¹ç¼–ç 

```python
from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token="[UNK]"))
tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)
tokenizer.normalizer = normalizers.Sequence(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)
print(tokenizer.normalizer.normalize_str("HÃ©llÃ² hÃ´w are Ã¼?"))
tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
pre_tokenizer = pre_tokenizers.Sequence(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]
trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
tokenizer.model = models.WordPiece(unk_token="[UNK]")
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
cls_token_id = tokenizer.token_to_id("[CLS]")
sep_token_id = tokenizer.token_to_id("[SEP]")
print(cls_token_id, sep_token_id)
tokenizer.post_processor = processors.TemplateProcessing(
    single=f"[CLS]:0 $A:0 [SEP]:0",
    pair=f"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1",
    special_tokens=[("[CLS]", cls_token_id), ("[SEP]", sep_token_id)],
)
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences.")
print(encoding.tokens)
print(encoding.type_ids)
tokenizer.decoder = decoders.WordPiece(prefix="##")
tokenizer.decode(encoding.ids)
tokenizer.save("tokenizer.json")
new_tokenizer = Tokenizer.from_file("tokenizer.json")

from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    # tokenizer_file="tokenizer.json", # You can load from the tokenizer file, alternatively
    unk_token="[UNK]",
    pad_token="[PAD]",
    cls_token="[CLS]",
    sep_token="[SEP]",
    mask_token="[MASK]",
)

from transformers import BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)

```

## æ„å»ºWordPieceç¼–ç 

```python
tokenizer = Tokenizer(models.BPE())
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test pre-tokenization!")
trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=["<|endoftext|>"])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
tokenizer.model = models.BPE()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)
sentence = "Let's test this tokenizer."
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[4]
sentence[start:end]
tokenizer.decoder = decoders.ByteLevel()
tokenizer.decode(encoding.ids)

from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<|endoftext|>",
    eos_token="<|endoftext|>",
)

from transformers import GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)
```

## æ„å»ºUnigramç¼–ç 

```python
tokenizer = Tokenizer(models.Unigram())

from tokenizers import Regex

tokenizer.normalizer = normalizers.Sequence(
    [
        normalizers.Replace("``", '"'),
        normalizers.Replace("''", '"'),
        normalizers.NFKD(),
        normalizers.StripAccents(),
        normalizers.Replace(Regex(" {2,}"), " "),
    ]
)
tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test the pre-tokenizer!")

special_tokens = ["<cls>", "<sep>", "<unk>", "<pad>", "<mask>", "<s>", "</s>"]
trainer = trainers.UnigramTrainer(
    vocab_size=25000, special_tokens=special_tokens, unk_token="<unk>"
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)

tokenizer.model = models.Unigram()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)

encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)

cls_token_id = tokenizer.token_to_id("<cls>")
sep_token_id = tokenizer.token_to_id("<sep>")
print(cls_token_id, sep_token_id)

tokenizer.post_processor = processors.TemplateProcessing(
    single="$A:0 <sep>:0 <cls>:2",
    pair="$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2",
    special_tokens=[("<sep>", sep_token_id), ("<cls>", cls_token_id)],
)

encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences!")
print(encoding.tokens)
print(encoding.type_ids)

tokenizer.decoder = decoders.Metaspace()

from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<s>",
    eos_token="</s>",
    unk_token="<unk>",
    pad_token="<pad>",
    cls_token="<cls>",
    sep_token="<sep>",
    mask_token="<mask>",
    padding_side="left",
)

from transformers import XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)
```

