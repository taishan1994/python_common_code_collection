## ä¸€èˆ¬æ“ä½œ

ç›´æ¥çœ‹ä»£ç ï¼š

```python
from datasets import load_dataset
from datasets import list_datasets
from pprint import pprint

# ç”¨äºå±•ç¤ºå¯ç”¨çš„datasets
pprint(list_datasets())

# åŠ è½½datasets
datasets = load_dataset("madao33/new-title-chinese")

# æŒ‡å®šæ•°æ®
datasets = load_dataset("madao33/new-title-chinese", split="train")
print(datasets)

# å±•ç¤ºæ•°æ®
datasets = load_dataset("madao33/new-title-chinese")
# ä»¥å­—å…¸çš„å½¢å¼è¿”å›æ•°æ®ï¼Œé”®ä¸ºç±»å‹ï¼Œå€¼ä¸ºåˆ—è¡¨
print(datasets["train"][:2])

# è‡ªå·±åˆ’åˆ†è®­ç»ƒæµ‹è¯•æ•°æ®
datasets = load_dataset("madao33/new-title-chinese")
datasets = datasets["train"]
datasets = datasets.train_test_split(test_size=0.1)
print(datasets)

# æ•°æ®çš„é€‰æ‹©å’Œè¿‡æ»¤
datasets = load_dataset("madao33/new-title-chinese")
# é€šè¿‡selectæ ¹æ®ç´¢å¼•é€‰æ‹©
select_datasets = datasets["train"].select([0, 2])
for example in select_datasets:
    # æ¯ä¸€æ¡ä»¥å­—å…¸çš„å½¢å¼è¿”å›
    print(example)
# é€šè¿‡filterè¿›è¡Œè¿‡æ»¤é€‰æ‹©
filter_datasets = datasets["train"].filter(lambda example: "ä¸­å›½" in example["title"])
for example in filter_datasets:
    print(example)
    break
    
é€šè¿‡mapå¯¹æ¯ä¸€æ¡æ•°æ®è¿›è¡Œå¤„ç†
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained(r"C:\Users\Administrator\Desktop\src\ner\model_hub\ner")


def process(example):
    inputs = tokenizer(example["title"],
                       max_length=64,
                       truncation=True,
                       padding="max_length")
    return inputs


processed_datasets = datasets.map(process, batched=True)
print(processed_datasets)
print(processed_datasets["train"][0])

# ä¿å­˜å’ŒåŠ è½½æ•°æ®
from datasets import load_from_disk

processed_datasets.save_to_disk("./news_data")
datasets = load_from_disk("./news_data")
print(datasets)
```

åŠ è½½è‡ªå®šä¹‰çš„æ•°æ®ï¼Œå¯ä»¥åŠ è½½jsonã€csvç­‰æ ¼å¼çš„æ•°æ®ï¼Œä¹Ÿå¯ä»¥zidingpyæ–‡ä»¶æ¥åŠ è½½æ•°æ®ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ï¼Œä½¿ç”¨jsonæˆ–è€…csvæ ¼å¼çš„æ•°æ®è¾ƒä¸ºç®€å•æ–¹ä¾¿ã€‚

jsonæ•°æ®çš„æ ¼å¼ï¼š

```json
{
  "version": "v1.0",
  "data": [
    {
      "title": "æœ›æµ·æ¥¼ç¾å›½æ‰“â€œå°æ¹¾ç‰Œâ€æ˜¯å±é™©çš„èµŒ",
      "content": "è¿‘æœŸï¼Œç¾å›½å›½ä¼šä¼—é™¢é€šè¿‡æ³•æ¡ˆï¼Œé‡ç”³ç¾å›½å¯¹å°æ¹¾çš„æ‰¿è¯ºã€‚"
    },
    {
      "title": "æœ›æµ·æ¥¼ç¾å›½æ‰“â€œå°æ¹¾ç‰Œâ€æ˜¯å±é™©çš„èµŒ",
      "content": "è¿‘æœŸï¼Œç¾å›½å›½ä¼šä¼—é™¢é€šè¿‡æ³•æ¡ˆï¼Œé‡ç”³ç¾å›½å¯¹å°æ¹¾çš„æ‰¿è¯ºã€‚"
    }
  ]
}

```

åŠ è½½æ•°æ®ä»£ç ï¼š

```python
# æ ¼å¼åŒ–åŠ è½½æ•°æ®
datasets = load_dataset("json", data_files=["data.json"], field="data")
print(datasets)
print(datasets["train"][:2])
```

```
DatasetDict({
    train: Dataset({
        features: ['title', 'content'],
        num_rows: 2
    })
})
{'title': ['æœ›æµ·æ¥¼ç¾å›½æ‰“â€œå°æ¹¾ç‰Œâ€æ˜¯å±é™©çš„èµŒ', 'æœ›æµ·æ¥¼ç¾å›½æ‰“â€œå°æ¹¾ç‰Œâ€æ˜¯å±é™©çš„èµŒ'], 'content': ['è¿‘æœŸï¼Œç¾å›½å›½ä¼šä¼—é™¢é€šè¿‡æ³•æ¡ˆï¼Œé‡ç”³ç¾å›½å¯¹å°æ¹¾çš„æ‰¿è¯ºã€‚', 'è¿‘æœŸï¼Œç¾å›½å›½ä¼šä¼—é™¢é€šè¿‡æ³•æ¡ˆï¼Œé‡ç”³ç¾å›½å¯¹å°æ¹¾çš„æ‰¿è¯ºã€‚']}
```

æŸ¥çœ‹å­—æ®µä¿¡æ¯ï¼š

```python
datasets["train"].features
```

æ‰“ä¹±æ•°æ®å¹¶é€‰å–éƒ¨åˆ†æ•°æ®ï¼š

```python
datasets = datasets["train"].shuffle(seed=42).select(range(1000))
```

## å¤„ç†å¤§æ•°æ®é›†

æŸ¥çœ‹datasetså ç”¨å†…å­˜ä»¥åŠæœ¬åœ°ç¼“å­˜å¤§å°:

```python
!pip install psutil
!pip install zstandard

from datasets import load_dataset

# This takes a few minutes to run, so go grab a tea or coffee while you wait :)
data_files = "https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst"
pubmed_dataset = load_dataset("json", data_files=data_files, split="train")
pubmed_dataset

import psutil

# Process.memory_info is expressed in bytes, so convert to megabytes
print(f"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB")

print(f"Number of files in dataset : {pubmed_dataset.dataset_size}")
size_gb = pubmed_dataset.dataset_size / (1024**3)
print(f"Dataset size (cache file) : {size_gb:.2f} GB")
```

å¦‚æœä½ ç†Ÿæ‚‰Pandasï¼Œè¿™ä¸ªç»“æœå¯èƒ½ä¼šè®©ä½ åƒæƒŠï¼Œå› ä¸ºWes Kinneyçš„è‘—åç»éªŒæ³•åˆ™æ˜¯ï¼šä½ é€šå¸¸éœ€è¦5åˆ°10å€äºdatasetså¤§å°çš„å†…å­˜ã€‚é‚£ä¹ˆï¼ŒğŸ¤—datasetsæ˜¯å¦‚ä½•è§£å†³è¿™ä¸ªå†…å­˜ç®¡ç†é—®é¢˜çš„å‘¢ï¼ŸğŸ¤—datasetså°†æ¯ä¸ªdatasetsè§†ä¸ºå†…å­˜æ˜ å°„çš„æ–‡ä»¶ï¼Œå®ƒæä¾›äº†RAMå’Œæ–‡ä»¶ç³»ç»Ÿå­˜å‚¨ä¹‹é—´çš„æ˜ å°„ï¼Œå…è®¸åº“è®¿é—®å’Œæ“ä½œdatasetsçš„å…ƒç´ ï¼Œè€Œä¸éœ€è¦å°†å…¶å®Œå…¨åŠ è½½åˆ°å†…å­˜ã€‚

å†…å­˜æ˜ å°„çš„æ–‡ä»¶ä¹Ÿå¯ä»¥åœ¨å¤šä¸ªè¿›ç¨‹ä¸­å…±äº«ï¼Œè¿™ä½¿å¾—Dataset.map()ç­‰æ–¹æ³•å¯ä»¥è¢«å¹¶è¡ŒåŒ–ï¼Œè€Œä¸éœ€è¦ç§»åŠ¨æˆ–å¤åˆ¶datasetsã€‚åœ¨å¼•æ“ç›–ä¸‹ï¼Œè¿™äº›åŠŸèƒ½éƒ½æ˜¯ç”±Apache Arrowå†…å­˜æ ¼å¼å’Œpyarrowåº“å®ç°çš„ï¼Œè¿™ä½¿å¾—æ•°æ®çš„åŠ è½½å’Œå¤„ç†å¿«å¦‚é—ªç”µã€‚ å…³äºApache Arrowçš„æ›´å¤šç»†èŠ‚ä»¥åŠä¸Pandasçš„æ¯”è¾ƒï¼Œè¯·æŸ¥çœ‹Dejan Simicçš„åšæ–‡ï¼‰ã€‚ä¸ºäº†çœ‹çœ‹è¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬åœ¨PubMed Abstracts datasetsä¸­çš„æ‰€æœ‰å…ƒç´ ä¸Šè¿›è¡Œä¸€æ¬¡å°çš„é€Ÿåº¦æµ‹è¯•ï¼š

```python
import timeit

code_snippet = """batch_size = 1000

for idx in range(0, len(pubmed_dataset), batch_size):
    _ = pubmed_dataset[idx:idx + batch_size]
"""

time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())
print(
    f"Iterated over {len(pubmed_dataset)} examples (about {size_gb:.1f} GB) in "
    f"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s"
)
```

'Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB/s'

è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨Pythonçš„timeitæ¨¡å—æ¥æµ‹é‡code_snippetçš„æ‰§è¡Œæ—¶é—´ã€‚ä½ é€šå¸¸èƒ½å¤Ÿä»¥ååˆ†ä¹‹å‡ åˆ°å‡ GB/sçš„é€Ÿåº¦è¿­ä»£ä¸€ä¸ªæ•°æ®é›†ã€‚è¿™å¯¹ç»å¤§å¤šæ•°åº”ç”¨æ¥è¯´éƒ½æ˜¯å¾ˆå¥½çš„ï¼Œä½†æœ‰æ—¶ä½ ä¸å¾—ä¸å¤„ç†ä¸€ä¸ªå¤§åˆ°ç”šè‡³æ— æ³•å­˜å‚¨åœ¨ä½ çš„ç¬”è®°æœ¬ç¡¬ç›˜ä¸Šçš„æ•°æ®é›†ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬è¯•å›¾ä¸‹è½½Pileçš„å…¨éƒ¨å†…å®¹ï¼Œæˆ‘ä»¬å°±éœ€è¦825GBçš„å¯ç”¨ç£ç›˜ç©ºé—´ ä¸ºäº†å¤„ç†è¿™äº›æƒ…å†µï¼ŒğŸ¤—æ•°æ®é›†æä¾›äº†ä¸€ä¸ªæµå¼åŠŸèƒ½ï¼Œå…è®¸æˆ‘ä»¬åœ¨é£è¡Œä¸­ä¸‹è½½å’Œè®¿é—®å…ƒç´ ï¼Œè€Œä¸éœ€è¦ä¸‹è½½æ•´ä¸ªæ•°æ®é›†ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹è¿™ä¸ªåŠŸèƒ½æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚

```python
pubmed_dataset_streamed = load_dataset(
    "json", data_files=data_files, split="train", streaming=True
)
```

æˆ‘ä»¬åœ¨æœ¬ç« å…¶ä»–åœ°æ–¹é‡åˆ°çš„æ˜¯ç†Ÿæ‚‰çš„Datasetï¼Œè€Œæµåª’ä½“=Trueè¿”å›çš„å¯¹è±¡æ˜¯ä¸€ä¸ªIterableDatasetã€‚é¡¾åæ€ä¹‰ï¼Œè¦è®¿é—®IterableDatasetçš„å…ƒç´ ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å®ƒè¿›è¡Œè¿­ä»£ã€‚æˆ‘ä»¬å¯ä»¥æŒ‰å¦‚ä¸‹æ–¹å¼è®¿é—®æˆ‘ä»¬çš„æµå¼æ•°æ®é›†çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼š

```python
next(iter(pubmed_dataset_streamed))
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

æ¥è‡ªæµæ•°æ®é›†çš„å…ƒç´ å¯ä»¥ä½¿ç”¨IterableDataset.map()è¿›è¡Œå³æ—¶å¤„ç†ï¼Œå¦‚æœä½ éœ€è¦å¯¹è¾“å…¥è¿›è¡Œæ ‡è®°ï¼Œé‚£ä¹ˆåœ¨è®­ç»ƒæœŸé—´æ˜¯éå¸¸æœ‰ç”¨çš„ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸æˆ‘ä»¬åœ¨ç¬¬ä¸‰ç« ä¸­ç”¨æ¥æ ‡è®°æ•°æ®é›†çš„è¿‡ç¨‹å®Œå…¨ç›¸åŒï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯è¾“å‡ºè¢«é€ä¸€è¿”å›ï¼š

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x["text"]))
next(iter(tokenized_dataset))
```

ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨IterableDataset.shuffle()å¯¹æµå¼æ•°æ®é›†è¿›è¡Œæ´—ç‰Œï¼Œä½†ä¸Dataset.shuffle()ä¸åŒï¼Œå®ƒåªå¯¹é¢„å®šä¹‰çš„ç¼“å†²åŒºå¤§å°çš„å…ƒç´ è¿›è¡Œæ´—ç‰Œï¼š

```python
shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)
next(iter(shuffled_dataset))
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä»ç¼“å†²åŒºçš„å‰10,000ä¸ªä¾‹å­ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªä¾‹å­ã€‚ä¸€æ—¦ä¸€ä¸ªä¾‹å­è¢«è®¿é—®ï¼Œå®ƒåœ¨ç¼“å†²åŒºä¸­çš„ä½ç½®å°±ä¼šè¢«è¯­æ–™åº“ä¸­çš„ä¸‹ä¸€ä¸ªä¾‹å­å¡«æ»¡ï¼ˆå³ä¸Šé¢ä¾‹å­ä¸­çš„ç¬¬10,001ä¸ªä¾‹å­ï¼‰ã€‚ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨IterableDataset.take()å’ŒIterableDataset.skip()å‡½æ•°ä»æµæ•°æ®é›†ä¸­é€‰æ‹©å…ƒç´ ï¼Œè¿™äº›å‡½æ•°çš„ä½œç”¨ä¸Dataset.select()ç±»ä¼¼ã€‚ä¾‹å¦‚ï¼Œä¸ºäº†é€‰æ‹©PubMed Abstractsæ•°æ®é›†ä¸­çš„å‰5ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·åšï¼š

```python
dataset_head = pubmed_dataset_streamed.take(5)
list(dataset_head)

[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'pmid': 11409575, 'language': 'eng'},
  'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...'},
 {'meta': {'pmid': 11409576, 'language': 'eng'},
  'text': "Hypoxaemia in children with severe pneumonia in Papua New Guinea ..."},
 {'meta': {'pmid': 11409577, 'language': 'eng'},
  'text': 'Oxygen concentrators and cylinders ...'},
 {'meta': {'pmid': 11409578, 'language': 'eng'},
  'text': 'Oxygen supply in rural africa: a personal experience ...'}]
```

åŒæ ·ï¼Œä½ å¯ä»¥ä½¿ç”¨IterableDataset.skip()å‡½æ•°ä»ä¸€ä¸ªæ´—è¿‡çš„æ•°æ®é›†ä¸­åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯åˆ†ç‰‡ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
# Skip the first 1,000 examples and include the rest in the training set
train_dataset = shuffled_dataset.skip(1000)
# Take the first 1,000 examples for the validation set
validation_dataset = shuffled_dataset.take(1000)
```

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªå¸¸è§çš„åº”ç”¨æ¥å®Œæˆæˆ‘ä»¬å¯¹æ•°æ®é›†æµçš„æ¢ç´¢ï¼šå°†å¤šä¸ªæ•°æ®é›†ç»„åˆåœ¨ä¸€èµ·ï¼Œåˆ›å»ºä¸€ä¸ªå•ä¸€çš„è¯­æ–™åº“ã€‚Datasetsæä¾›äº†ä¸€ä¸ªinterleave_datasets()å‡½æ•°ï¼Œå¯ä»¥å°†ä¸€ä¸ªIterableDatasetå¯¹è±¡çš„åˆ—è¡¨è½¬æ¢æˆä¸€ä¸ªIterableDatasetï¼Œå…¶ä¸­æ–°æ•°æ®é›†çš„å…ƒç´ æ˜¯é€šè¿‡äº¤æ›¿ä½¿ç”¨æºå®ä¾‹è·å¾—çš„ã€‚å½“ä½ è¯•å›¾ç»“åˆå¤§å‹æ•°æ®é›†æ—¶ï¼Œè¿™ä¸ªå‡½æ•°ç‰¹åˆ«æœ‰ç”¨ï¼Œæ‰€ä»¥ä½œä¸ºä¸€ä¸ªä¾‹å­ï¼Œè®©æˆ‘ä»¬æŠŠPileçš„FreeLawå­é›†æµå‡ºæ¥ï¼Œè¿™æ˜¯ä¸€ä¸ª51GBçš„ç¾å›½æ³•é™¢çš„æ³•å¾‹æ„è§çš„æ•°æ®é›†ï¼š

```python
law_dataset_streamed = load_dataset(
    "json",
    data_files="https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst",
    split="train",
    streaming=True,
)
next(iter(law_dataset_streamed))

{'meta': {'case_ID': '110921.json',
  'case_jurisdiction': 'scotus.tar.gz',
  'date_created': '2010-04-28T17:12:49Z'},
 'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}
```

è¿™ä¸ªæ•°æ®é›†å¤§åˆ°è¶³ä»¥ç»™å¤§å¤šæ•°ç¬”è®°æœ¬ç”µè„‘çš„å†…å­˜å¸¦æ¥å‹åŠ›ï¼Œä½†æˆ‘ä»¬å´èƒ½åœ¨ä¸å‡ºæ±—çš„æƒ…å†µä¸‹åŠ è½½å’Œè®¿é—®å®ƒ ç°åœ¨è®©æˆ‘ä»¬ç”¨interleave_datasets()å‡½æ•°å°†FreeLawå’ŒPubMed Abstractsæ•°æ®é›†çš„ä¾‹å­ç»“åˆèµ·æ¥ï¼š

```python
from itertools import islice
from datasets import interleave_datasets

combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])
list(islice(combined_dataset, 2))

```

è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº†Pythonçš„itertoolsæ¨¡å—ä¸­çš„islice()å‡½æ•°ï¼Œä»åˆå¹¶çš„æ•°æ®é›†ä¸­é€‰æ‹©äº†å‰ä¸¤ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒä»¬ä¸ä¸¤ä¸ªæºæ•°æ®é›†ä¸­çš„ç¬¬ä¸€ä¸ªä¾‹å­ç›¸åŒ¹é…ã€‚

æœ€åï¼Œå¦‚æœä½ æƒ³æŠŠ825GBçš„Pileå…¨éƒ¨ä¸²è”èµ·æ¥ï¼Œä½ å¯ä»¥æŒ‰ä»¥ä¸‹æ–¹å¼æŠ“å–æ‰€æœ‰å‡†å¤‡å¥½çš„æ–‡ä»¶ï¼š

```python
base_url = "https://the-eye.eu/public/AI/pile/"
data_files = {
    "train": [base_url + "train/" + f"{idx:02d}.jsonl.zst" for idx in range(30)],
    "validation": base_url + "val.jsonl.zst",
    "test": base_url + "test.jsonl.zst",
}
pile_dataset = load_dataset("json", data_files=data_files, streaming=True)
next(iter(pile_dataset["train"]))
```

ä½ ç°åœ¨æ‹¥æœ‰æ‰€æœ‰ä½ éœ€è¦çš„å·¥å…·æ¥åŠ è½½å’Œå¤„ç†å„ç§å½¢çŠ¶å’Œå¤§å°çš„æ•°æ®é›†--ä½†é™¤éä½ ç‰¹åˆ«å¹¸è¿ï¼Œå¦åˆ™åœ¨ä½ çš„NLPæ—…ç¨‹ä¸­ä¼šæœ‰ä¸€ä¸ªç‚¹ï¼Œä½ å¿…é¡»å®é™…åˆ›å»ºä¸€ä¸ªæ•°æ®é›†æ¥è§£å†³æ‰‹å¤´çš„é—®é¢˜ã€‚è¿™å°±æ˜¯ä¸‹ä¸€èŠ‚çš„ä¸»é¢˜!
